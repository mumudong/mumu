spark特征选择常用工具:
     http://blog.csdn.net/chenguangchun1993/article/details/78840577
特征提取
    TF-IDF
        在文本挖掘中广泛使用的特征向量化方法，它可以体现一个文档中词语在语料库中的重要程度。
        词频TF(t,d)是词语t在文档d中出现的次数。文件频率DF(t,D)是包含词语的文档的个数。
        如果我们只使用词频来衡量重要性，很容易过度强调在文档中经常出现，却没有太多实际信息的词语，
            比如“a”，“the”以及“of”。如果一个词语经常出现在语料库中，意味着它并不能很好的对文档进行区分。
        当词出现在所有文档中时，它的IDF值变为0。加1是为了避免分母为0的情况。
        IDF(t,D)=log((|D|+1)/(DF(t,D)+1))
        TFIDF(t,d,D)=TF(td)⋅IDF(t,D)
        在Spark ML库中，TF-IDF被分成两部分：TF(HashingTF) 和 IDF
            HashingTF 是一个Transformer，在文本处理中，接收词条的集合然后把这些集合转化成固定长度的特征向量。
                这个算法在哈希的同时会统计各个词条的词频。
            IDF是一个Estimator，在一个数据集上应用它的fit（）方法，产生一个IDFModel。 该IDFModel 接收特征向量
                （由HashingTF产生），然后计算每一个词在文档中出现的频次。IDF会减少那些在语料库中出现频率较高的
                词的权重。
     OneHotEncoder
          独热编码是指把一列标签索引映射成一列二进制数组,默认不包含最后一个类别
          OneHotEncoder.setDropLast(false)可以设置是否包含最后一个类别
      VectorIndexer
            解决向量数据集中的类别特征索引。
            它可以自动识别哪些特征是类别型(离散)的，并且将原始值转换为类别索引。
            设置maxCategories为2，小于2的特征才被认为是类别型特征，否被认为是连续型特征。
        Normalizer
            将数据集的每一行数据归一化的转换器，一行数据看做一个向量，它带一个参数P（默认2），
              改参数决定采用什么范数进行归一化，（默认L2,几平方和开根）
              val data = Seq(Vectors.dense(-1.0, 1.0, 1.0),Vectors.dense(-1.0, 3.0, 1.0), Vectors.dense(0.0, 5.0, 1.0))
              val df = sqlContext.createDataFrame(data.map(Tuple1.apply)).toDF("features")
              val normalizer = new Normalizer()
                .setInputCol("features")
                .setOutputCol("normFeatures")
                .setP(2.0)
              val l1NormData = normalizer.transform(dataFrame)
              l1NormData.show()
              //使用无穷范数，即绝对值最大值
                val lInfNormData = normalizer.transform(dataFrame, normalizer.p -> Double.PositiveInfinity)
                lInfNormData.show(false)
        VectorAssembler
            作用很简单，就是把DataFrame中的若干列合并为一个向量列
            val dataset = sqlcontext.createDataFrame(
              Seq((0, 18, 1.0, Vectors.dense(0.0, 10.0, 0.5), 1.0))
            ).toDF("id", "hour", "mobile", "userFeatures", "clicked")

            val assembler = new VectorAssembler()
              .setInputCols(Array("hour", "mobile", "userFeatures"))
              .setOutputCol("features")

            val output = assembler.transform(dataset)
            output.show(false)
        VectorSlicer
            切片工具，和VectorAssembler相对应，它的作用就是将一个向量切片，选择向量的子集。它提供了两种切片方式。
                setIndices(), 按照向量的下标索引切片(从0开始)
                setNames()，按照列名切片
                val data = Array(Row(Vectors.dense(-2.0, 2.3, 0.0)))
                val defaultAttr = NumericAttribute.defaultAttr
                val attrs = Array("f1", "f2", "f3").map(defaultAttr.withName)
                val attrGroup = new AttributeGroup("userFeatures", attrs.asInstanceOf[Array[Attribute]])
                val dataRDD = sc.parallelize(data)
                val dataset = sqlContext.createDataFrame(dataRDD, StructType(Array(attrGroup.toStructField())))
                val slicer = new VectorSlicer().setInputCol("userFeatures").setOutputCol("features")
                slicer.setIndices(Array(1)).setNames(Array("f3"))
                // or slicer.setIndices(Array(1, 2)), or slicer.setNames(Array("f2", "f3"))
                val output = slicer.transform(dataset)
                ouput.show(false)








