==========================
特征选择是特征工程中的重要问题（另一个重要的问题是特征提取），坊间常说：数据和特征决定了机器学习的上限，
而模型和算法只是逼近这个上限而已。由此可见，特征工程尤其是特征选择在机器学习中占有相当重要的地位

1. 计算每一个特征与响应变量的相关性：工程上常用的手段有计算皮尔逊系数和互信息系数，皮尔逊系数只能衡量
   线性相关性而互信息系数能够很好地度量各种相关性，但是计算相对复杂一些，好在很多toolkit里边都包含了这个
   工具（如sklearn的MINE），得到相关性之后就可以排序选择特征了；
2. 构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征，另外，记得JMLR'03上有一篇论文介绍
   了一种基于决策树的特征选择方法，本质上是等价的。当选择到了目标特征之后，再用来训练最终的模型；
3. 通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，
   L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要
   应再通过L2正则方法交叉检验；
4. 训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分
   获得相关性后再训练最终模型；
5. 通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在
   推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，
   组合特征能够同时兼顾全局模型和个性化模型，这个问题有机会可以展开讲。
6. 通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，
   原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。
   从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。整体上来说，特征选择是一个
   既有学术价值又有工程价值的问题，目前在研究领域也比较热，值得所有做机器学习的朋友重视。
===========================
1，特征值分解 和 奇异值分解
    矩阵可以看做运动，包括旋转，拉伸，投影三个操作。

    首先，矩阵可以认为是一种线性变换，而且这种线性变换的作用效果与基的选择有关。以Ax = b为例，x是m维向量，
b是n维向量，m,n可以相等也可以不相等，表示矩阵可以将一个向量线性变换到另一个向量，这样一个线性变换的作用
可以包含旋转、缩放和投影三种类型的效应。
    奇异值分解正是对线性变换这三种效应的一个析构。A=U∑V，u和v是两组正交单位向量，是对角阵，表示奇异值，它表示我们找到
了和这样两组基，A矩阵的作用是将一个向量从u这组正交基向量的空间旋转到v这组正交基向量空间，并对每个方向进行了一定
的缩放，缩放因子就是各个奇异值。如果维度比大，则表示还进行了投影。可以说奇异值分解将一个矩阵原本混合在一起的
三种作用效果，分解出来了。
    而特征值分解其实是对旋转缩放两种效应的归并。（有投影效应的矩阵不是方阵，没有特征值）特征值，特征向量由
Ax=x得到，它表示如果一个向量v处于A的特征向量方向，那么Av对v的线性变换作用只是一个缩放。也就是说，求特征向量和
特征值的过程，我们找到了这样一组基，在这组基下，矩阵的作用效果仅仅是存粹的缩放。对于实对称矩阵，特征向量正交，
我们可以将特征向量式子写成，这样就和奇异值分解类似了，就是A矩阵将一个向量从x这组基的空间旋转到x这组基的空间，
并在每个方向进行了缩放，由于前后都是x，就是没有旋转或者理解为旋转了0度。
总结一下，特征值分解和奇异值分解都是给一个矩阵(线性变换)找一组特殊的基，特征值分解找到了特征向量这组基，在这组基
下该线性变换只有缩放效果。而奇异值分解则是找到另一组基，这组基下线性变换的旋转、缩放、投影三种功能独立地展示出来
了。我感觉特征值分解其实是一种找特殊角度，让旋转效果不显露出来，所以并不是所有矩阵都能找到这样巧妙的角度。仅有缩放
效果，表示、计算的时候都更方便，这样的基很多时候不再正交了，又限制了一些应用。

    QR分解:如果实（复）非奇异矩阵A能够化成正交（酉）矩阵Q与实（复）非奇异上三角矩阵R的乘积，即A=QR，
则称其为A的QR分解。
    QR（正交三角）分解法是目前求一般矩阵全部特征值的最有效并广泛应用的方法，一般矩阵先经过正交相似
    变化成为Hessenberg矩阵，然后再应用QR方法求特征值和特征向量。它是将矩阵分解成一个正规正交矩阵Q与
    上三角形矩阵R，所以称为QR分解法，与此正规正交矩阵的通用符号Q有关。

    奇异矩阵:首先，看这个矩阵是不是方阵（即行数和列数相等的矩阵。若行数和列数不相等，那就谈不上
    奇异矩阵和非奇异矩阵）。 然后，再看此矩阵的行列式|A|是否等于0，若等于0，称矩阵A为奇异矩阵；
    若不等于0，称矩阵A为非奇异矩阵。

三角分解法是将原正方 (square) 矩阵分解成一个上三角形矩阵或是排列(permuted) 的上三角形矩阵和一个 下三角形矩阵，
这样的分解法又称为LU分解法。它的用途主要在简化一个大矩阵的行列式值的计算过程，求逆矩阵，和求解联立方程组。
不过要注意这种分解法所得到的上下三角形矩阵并非唯一，还可找到数个不同 的一对上下三角形矩阵，此两三角形矩阵相乘
也会得到原矩阵。

QR分解法是将矩阵分解成一个正规正交矩阵与上三角形矩阵,所以称为QR分解法,与此正规正交矩阵的通用符号Q有关。

奇异值分解 (singular value decomposition,SVD) 是另一种正交矩阵分解法；SVD是最可靠的分解法，但是它比QR
分解法要花上近十倍的计算时间。[U,S,V]=svd(A)，其中U和V分别代表两个正交矩阵，而S代表一对角矩阵。
和QR分解法相同， 原矩阵A不必为正方矩阵。使用SVD分解法的用途是解最小平方误差法和数据压缩。



机器学习要注意：海量离散特征+简单模型组合
            或者  少量连续特征+复杂模型组合
           的权衡，一个折腾特征，一个折腾模型，注意权衡，第一个已广泛使用，第二个前景看起来很好。

==========================================
2，范数
向量范数:
    1-范数：即向量元素绝对值之和，matlab调用函数norm(x, 1) 。
    2-范数：Euclid范数（欧几里得范数，常用计算向量长度），即向量元素绝对值的平方和再开方，matlab调用函数norm(x, 2)。
    \infty-范数,无穷大范数:即所有向量元素绝对值中的最大值，matlab调用函数norm(x, inf)。
    -\infty-范数，负无穷大范数：即所有向量元素绝对值中的最小值，matlab调用函数norm(x, -inf)。
    p-范数：即向量元素绝对值的p次方和的1/p次幂，matlab调用函数norm(x, p)。
矩阵范数：
    1-范数：列和范数，即所有矩阵列向量绝对值之和的最大值，matlab调用函数norm(A, 1)。
    2-范数：谱范数，即A'A矩阵的最大特征值的开平方。matlab调用函数norm(x, 2)。
    \infty-范数：行和范数，即所有矩阵行向量绝对值之和的最大值，matlab调用函数norm(A, inf)。
    F-范数：Frobenius范数，即矩阵元素绝对值的平方和再开平方，matlab调用函数norm(A, ’fro‘)。
    核范数：即奇异值之和
=========================================
3，协方差
当两个变量相关时，用于评估它们因相关而产生的对应变量的影响。
当多个变量独立时，用方差来评估这种影响的差异。
当多个变量相关时，用协方差来评估这种影响的差异。
==========================================
4,大数定理，中心极限定理
中心极限定理是说：和的分布收敛于正态分布
    样本的平均值约等于总体的平均值。
    不管总体是什么分布，任意一个总体的样本平均值都会围绕在总体的整体平均值周围，并且呈正态分布。
小数定律：
    如果统计数据很少，那么事件就表现为各种极端情况，而这些情况都是偶然事件，跟它的期望值一点关系都没有。
如果数据少，随机现象可以看上去很不随机。甚至非常整齐，感觉好像真有规律一样。

大数定律：频率收敛于概率
    如果统计数据足够大，那么事物出现的频率就能无限接近他的期望值。
===========================================
离散数据--概率函数
连续数据--概率密度函数
概率分布：http://blog.csdn.net/dymodi/article/details/54231728
    正态曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，因此人们又经常称之为钟形曲线。
    若随机变量X服从一个数学期望为μ、方差为σ^2的正态分布，记为N(μ，σ^2)。其概率密度函数为
正态分布的期望值μ决定了其位置，其标准差σ决定了分布的幅度。当μ = 0,σ = 1时的正态分布是标准正态分布。

    Thin-tail distribution:
    Heavy-tailed distribution:
    指数分布在x->infinity时，以指数的速度趋近于0，以指数分布为分界线，下降速度更快的
称为Thin-tail distribution如正态分布，在远离峰值的尾部区域，时间发生的概率更低一些。
所以正态分布用来对那些主流事件发生较多，非主流事件发生较少的情况进行建模更为合适。
    x->infinity时下降速度慢于指数分布的称为重尾分布Heavy-tailed distribution重尾分布
更适用于对那些离峰值较远的稀有事件也会有相当的概率发生的情况。重尾分布作为一个大的类别，
还包含三个重要的子类别，分别是肥尾分布（Fat-tailed distribution），长尾分布（Long-tailed distribution）
和次指数分布（Subexponential distribution）。
    长尾分布，或者说长尾理论是一个与互联网发展分不开的概念。说到这里就不得不先提一下传统商业中的
帕累托法则（Pareto principle），又称为二八定律。比如80%的财富集中在20%的人手里，图书馆里20%的书
可以满足80%的顾客。于是大家往往只关注在PDF图中最左面的20%的顾客，以期满足80%，如下图绿色的部分，
来实现效益的最大化。
    但在一些网上零售业中，如Amazon和Netflix，数据表明右端黄色的尾巴虽然平均需求小但是由于数量巨大，
导致其总的营销收益甚至超过主流的商品。这一发现似乎对商业界的触动极大，也说明了正确建模的重要性。
如果用指数分布进行建模，这些远端的需求也许就会被忽视；而用长尾分布进行建模就可以发现这些新的需求
从而带来效益的提高。
==========================================
插值和拟合：
    插值：是指已知某函数的在若干离散点上的函数值或者导数信息，通过求解该函数中待定形式的插值函数以及
待定系数，使得该函数在给定离散点上满足约束。
    拟合：是指已知某函数的若干离散函数值{f1,f2,…,fn}，通过调整该函数中若干待定系数f(λ1, λ2,…,λn),
使得该函数与已知点集的差别(最小二乘意义)最小。如果待定函数是线性，就叫线性拟合或者线性回归(主要在统计中)，
否则叫作非线性拟合或者非线性回归。表达式也可以是分段函数，这种情况下叫作样条拟合。
    从几何意义上将，拟合是给定了空间中的一些点，找到一个已知形式未知参数的连续曲面来最大限度地逼近这些点；
    而插值是找到一个( 或几个分片光滑的)连续曲面来穿过这些点。

==========================================
正定矩阵，半正定矩阵：
    半正定矩阵定义为: X^T M X >= 0
        其中X 是任意向量，M 是变换矩阵

考虑矩阵的特征值。
若所有特征值均不小于零，则称为半正定。
若所有特征值均大于零，则称为正定。

==========================================
向量内积和外积：
  外积：右手法则 ab sinc
  内积: ab cosc

==========================================
roc,auc,召回率
   卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定
卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合，若两个值完全相等时，卡方值就为0，
表明理论值完全符合。

为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。
在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的
正负样本的分布也可能随着时间变化。

准确率(accuracy),其定义是: 对于给定的测试数据集，分类器正确分类的样本数与总样本数之比

精确率(precision)的公式是P = {TP}/{TP+FP}  它计算的是所有"正确被检索的item(TP)"占所有"实际被检索到的(TP+FP)"的比例
召回率(recall)的公式是   R =  TP}/{TP+FN},它计算的是所有"正确被检索的item(TP)"占所有"应该检索到的item(TP+FN)"的比例。
==========================================
LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和PCA不同。
PCA是不考虑样本类别输出的无监督降维技术。LDA的思想可以用一句话概括，就是“投影后类内方差最小，
类间方差最大”。什么意思呢？ 我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点
尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。



==========================================
https://github.com/scikit-learn/scikit-learn/blob/master/examples/linear_model/plot_sgd_iris.py


