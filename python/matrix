===========================
1，特征值分解 和 奇异值分解
    矩阵可以看做运动，包括旋转，拉伸，投影三个操作。

    首先，矩阵可以认为是一种线性变换，而且这种线性变换的作用效果与基的选择有关。以Ax = b为例，x是m维向量，
b是n维向量，m,n可以相等也可以不相等，表示矩阵可以将一个向量线性变换到另一个向量，这样一个线性变换的作用
可以包含旋转、缩放和投影三种类型的效应。
    奇异值分解正是对线性变换这三种效应的一个析构。A=，和是两组正交单位向量，是对角阵，表示奇异值，它表示我们找到
了和这样两组基，A矩阵的作用是将一个向量从这组正交基向量的空间旋转到这组正交基向量空间，并对每个方向进行了一定
的缩放，缩放因子就是各个奇异值。如果维度比大，则表示还进行了投影。可以说奇异值分解将一个矩阵原本混合在一起的
三种作用效果，分解出来了。
    而特征值分解其实是对旋转缩放两种效应的归并。（有投影效应的矩阵不是方阵，没有特征值）特征值，特征向量由
Ax=x得到，它表示如果一个向量v处于A的特征向量方向，那么Av对v的线性变换作用只是一个缩放。也就是说，求特征向量和
特征值的过程，我们找到了这样一组基，在这组基下，矩阵的作用效果仅仅是存粹的缩放。对于实对称矩阵，特征向量正交，
我们可以将特征向量式子写成，这样就和奇异值分解类似了，就是A矩阵将一个向量从x这组基的空间旋转到x这组基的空间，
并在每个方向进行了缩放，由于前后都是x，就是没有旋转或者理解为旋转了0度。
总结一下，特征值分解和奇异值分解都是给一个矩阵(线性变换)找一组特殊的基，特征值分解找到了特征向量这组基，在这组基
下该线性变换只有缩放效果。而奇异值分解则是找到另一组基，这组基下线性变换的旋转、缩放、投影三种功能独立地展示出来
了。我感觉特征值分解其实是一种找特殊角度，让旋转效果不显露出来，所以并不是所有矩阵都能找到这样巧妙的角度。仅有缩放
效果，表示、计算的时候都更方便，这样的基很多时候不再正交了，又限制了一些应用。

    QR分解:如果实（复）非奇异矩阵A能够化成正交（酉）矩阵Q与实（复）非奇异上三角矩阵R的乘积，即A=QR，
则称其为A的QR分解。
    QR（正交三角）分解法是目前求一般矩阵全部特征值的最有效并广泛应用的方法，一般矩阵先经过正交相似
    变化成为Hessenberg矩阵，然后再应用QR方法求特征值和特征向量。它是将矩阵分解成一个正规正交矩阵Q与
    上三角形矩阵R，所以称为QR分解法，与此正规正交矩阵的通用符号Q有关。

    奇异矩阵:首先，看这个矩阵是不是方阵（即行数和列数相等的矩阵。若行数和列数不相等，那就谈不上
    奇异矩阵和非奇异矩阵）。 然后，再看此矩阵的行列式|A|是否等于0，若等于0，称矩阵A为奇异矩阵；
    若不等于0，称矩阵A为非奇异矩阵。

==========================================
2，范数
向量范数:
    1-范数：即向量元素绝对值之和，matlab调用函数norm(x, 1) 。
    2-范数：Euclid范数（欧几里得范数，常用计算向量长度），即向量元素绝对值的平方和再开方，matlab调用函数norm(x, 2)。
    \infty-范数,无穷大范数:即所有向量元素绝对值中的最大值，matlab调用函数norm(x, inf)。
    -\infty-范数，负无穷大范数：即所有向量元素绝对值中的最小值，matlab调用函数norm(x, -inf)。
    p-范数：即向量元素绝对值的p次方和的1/p次幂，matlab调用函数norm(x, p)。
矩阵范数：
    1-范数：列和范数，即所有矩阵列向量绝对值之和的最大值，matlab调用函数norm(A, 1)。
    2-范数：谱范数，即A'A矩阵的最大特征值的开平方。matlab调用函数norm(x, 2)。
    \infty-范数：行和范数，即所有矩阵行向量绝对值之和的最大值，matlab调用函数norm(A, inf)。
    F-范数：Frobenius范数，即矩阵元素绝对值的平方和再开平方，matlab调用函数norm(A, ’fro‘)。
    核范数：即奇异值之和
=========================================
3，协方差
当两个变量相关时，用于评估它们因相关而产生的对应变量的影响。
当多个变量独立时，用方差来评估这种影响的差异。
当多个变量相关时，用协方差来评估这种影响的差异。
==========================================
4,大数定理，中心极限定理
中心极限定理是说：
    样本的平均值约等于总体的平均值。
    不管总体是什么分布，任意一个总体的样本平均值都会围绕在总体的整体平均值周围，并且呈正态分布。
小数定律：
    如果统计数据很少，那么事件就表现为各种极端情况，而这些情况都是偶然事件，跟它的期望值一点关系都没有。
如果数据少，随机现象可以看上去很不随机。甚至非常整齐，感觉好像真有规律一样。

大数定律：
    如果统计数据足够大，那么事物出现的频率就能无限接近他的期望值。
==========================================
概率分布：http://blog.csdn.net/dymodi/article/details/54231728
    正态曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，因此人们又经常称之为钟形曲线。
    若随机变量X服从一个数学期望为μ、方差为σ^2的正态分布，记为N(μ，σ^2)。其概率密度函数为
正态分布的期望值μ决定了其位置，其标准差σ决定了分布的幅度。当μ = 0,σ = 1时的正态分布是标准正态分布。

    Thin-tail distribution:
    Heavy-tailed distribution:
    指数分布在x->infinity时，以指数的速度趋近于0，以指数分布为分界线，下降速度更快的
称为Thin-tail distribution如正态分布，在远离峰值的尾部区域，时间发生的概率更低一些。
所以正态分布用来对那些主流事件发生较多，非主流事件发生较少的情况进行建模更为合适。
    x->infinity时下降速度慢于指数分布的称为重尾分布Heavy-tailed distribution重尾分布
更适用于对那些离峰值较远的稀有事件也会有相当的概率发生的情况。重尾分布作为一个大的类别，
还包含三个重要的子类别，分别是肥尾分布（Fat-tailed distribution），长尾分布（Long-tailed distribution）
和次指数分布（Subexponential distribution）。
    长尾分布，或者说长尾理论是一个与互联网发展分不开的概念。说到这里就不得不先提一下传统商业中的
帕累托法则（Pareto principle），又称为二八定律。比如80%的财富集中在20%的人手里，图书馆里20%的书
可以满足80%的顾客。于是大家往往只关注在PDF图中最左面的20%的顾客，以期满足80%，如下图绿色的部分，
来实现效益的最大化。
    但在一些网上零售业中，如Amazon和Netflix，数据表明右端黄色的尾巴虽然平均需求小但是由于数量巨大，
导致其总的营销收益甚至超过主流的商品。这一发现似乎对商业界的触动极大，也说明了正确建模的重要性。
如果用指数分布进行建模，这些远端的需求也许就会被忽视；而用长尾分布进行建模就可以发现这些新的需求
从而带来效益的提高。
==========================================



==========================================



==========================================



==========================================



==========================================



==========================================
