txdb1 123456
管理员密码$$1imadmin6@

./pg_ctl restart -D ../data/ &

$txbi2017\^\^

\du; 查看用户及权限
create user txbi with superuser password 'txbipg';
主从复制-读写分离：
    主服务器
        先创建一个新目录：
            mkdir /opt/pgsql/pg_archive
        首先需要创建一个数据库用户进行主从同步。创建用户replica，并赋予登录和复制的权限。
            postgres# CREATE ROLE replica login replication encrypted password 'replica'
        在pg_hba.conf里增加两行：
            host     all             all          192.168.20.94/32          trust   #允许94连接到主服务器
            host   replication      replica       192.168.20.94/32          md5   #允许94使用replica用户来复制
        修改postgresql.conf
            listen_addresses = '*'   # 监听所有IP
            archive_mode = on  # 允许归档
            archive_command = 'cp %p /opt/pgsql/pg_archive/%f'  # 用该命令来归档logfile segment
            wal_level = hot_standby
            max_wal_senders = 32 # 这个设置了可以最多有几个流复制连接，差不多有几个从，就设置几个wal_keep_segments = 256 ＃ 设置流复制保留的最多的xlog数目
            wal_sender_timeout = 60s ＃ 设置流复制主机发送数据的超时时间
            max_connections = 100 # 这个设置要注意下，从库的max_connections必须要大于主库的
        配置完两个文件后重启服务器。
            pg_ctl stop -D /opt/pgsql/data
            pg_ctl start -D /opt/pgsql/data
从服务器：
        从主节点拷贝数据到从节点
            su - postgres
            rm -rf /opt/pgsql/data/*   #先将data目录下的数据都清空
            pg_basebackup -h 192.168.20.93 -U replica -D /opt/pgsql/data -X stream -P  # 从93拷贝数据到94（基础备份）
            mkdir /opt/pgsql/pg_archive
        配置recovery.conf
            复制/usr/pgsql-9.4/share/recovery.conf.sample 到 /opt/pgsql/data/recovery.conf
                cp /usr/pgsql-9.4/share/recovery.conf.sample /opt/pgsql/data/recovery.conf
            修改recovery.conf
                standby_mode = on    # 说明该节点是从服务器
                primary_conninfo = 'host=192.168.20.93 port=5432 user=replica password=replica'  # 主服务器的信息以及连接的用户
                recovery_target_timeline = 'latest'
            配置postgresql.conf
                wal_level = hot_standby
                max_connections = 1000 ＃ 一般查多于写的应用从库的最大连接数要比较大
                hot_standby = on ＃ 说明这台机器不仅仅是用于数据归档，也用于数据查询
                max_standby_streaming_delay = 30s # 数据流备份的最大延迟时间
                wal_receiver_status_interval = 10s # 多久向主报告一次从的状态，当然从每次数据复制都会向主报告状态，这里只是设置最长的间隔时间
                hot_standby_feedback = on # 如果有错误的数据复制，是否向主进行反馈
            配置完后重启从服务器
                pg_ctl stop -D /opt/pgsql/data
                pg_ctl start -D /opt/pgsql/data


3. 验证是否部署成功

在主节点上执行：

select client_addr,sync_state from pg_stat_replication;
结果如下：

postgres=# select client_addr,sync_state from pg_stat_replication;
  client_addr  | sync_state
---------------+------------
 192.168.20.94 | async
(1 行记录)
说明94是从服务器，在接收流，而且是异步流复制。

此外，还可以分别在主、从节点上运行 ps aux | grep postgres 来查看进程：



===========================================
测试:
-->:
    mysqlslap -hhdf-3 -P8066 --concurrency=1000 --iterations=1 --create-schema='TESTZB' --query='select * from pgbench_accounts where aid='9';' --number-of-queries=10000 --debug-info -uroot -p123456
性能测试及调优：

初始化数据库
    ./pgbench -i -s 20 test -U txdb_bi
测试 1---->
	模拟20个并发用户，每个用户执行2000次事务。每种配置参数执行三次，记录TPS值。
[txdb@hdf-3 bin]$ ./pgbench -t 2000 -c 20 -U txdb_bi test
    starting vacuum...end.
    transaction type: TPC-B (sort of)
    scaling factor: 20
    query mode: simple
    number of clients: 20
    number of threads: 1
    number of transactions per client: 2000
    number of transactions actually processed: 40000/40000
    latency average: 0.000 ms
    tps = 14458.676200 (including connections establishing)
    tps = 14466.893004 (excluding connections establishing)
注:TPS (transaction per second)

sysbench下载
http://imysql.com/wp-content/uploads/2014/09/sysbench-0.4.12-1.1.tgz
cd /tmp/sysbench-0.4.12-1.1
./autogen.sh
./configure --with-mysql-includes=/usr/local/mysql/include --with-mysql-libs=/usr/local/mysql/lib && make

# 如果 make 没有报错，就会在 sysbench 目录下生成二进制命令行工具 sysbench
ls -l sysbench
-rwxr-xr-x 1 root root 3293186 Sep 21 16:24 sysbench
mysqladmin -u root -p123456 create sbtest

./sysbench --mysql-host=hdf-1 --mysql-port=3306 --mysql-user=root --mysql-password=root123 \
 --test=tests/db/oltp.lua --oltp_tables_count=10 --oltp-table-size=100000 --rand-init=on prepare
        --test=tests/db/oltp.lua 表示调用 tests/db/oltp.lua 脚本进行 oltp 模式测试
        --oltp_tables_count=10 表示会生成 10 个测试表
        --oltp-table-size=100000 表示每个测试表填充数据量为 100000
        --rand-init=on 表示每个测试表都是用随机数据来填充的


/sysbench --mysql-host=hdf-1 --mysql-port=3306 --mysql-user=root \
--mysql-password=root123 --test=tests/db/oltp.lua --oltp_tables_count=10 \
--oltp-table-size=10000000 --num-threads=8 --oltp-read-only=true \
--report-interval=10 --rand-type=uniform --max-time=3600 \
 --max-requests=0 --percentile=99 run >> ./log/sysbench_oltpX_8_20140921.log

        --num-threads=8 表示发起 8个并发连接
        --oltp-read-only=off 表示不要进行只读测试，也就是会采用读写混合模式测试
        --report-interval=10 表示每10秒输出一次测试进度报告
        --rand-type=uniform 表示随机类型为固定模式，其他几个可选随机模式：uniform(固定),gaussian(高斯),special(特定的),pareto(帕累托)
        --max-time=120 表示最大执行时长为 120秒
        --max-requests=0 表示总请求数为 0，因为上面已经定义了总执行时长，所以总请求数可以设定为 0；也可以只设定总请求数，不设定最大执行时长
        --percentile=99 表示设定采样比例，默认是 95%，即丢弃1%的长请求，在剩余的99%里取最大值
        即：模拟 对10个表并发OLTP测试，每个表1000万行记录，持续压测时间为 1小时。
===========================================
性能测试及调优：

初始化数据库
    ./pgbench -i -s 20 test -U txdb_bi
测试 1---->
	模拟20个并发用户，每个用户执行2000次事务。每种配置参数执行三次，记录TPS值。
[txdb@hdf-3 bin]$ ./pgbench -t 2000 -c 20 -U txdb_bi test
    starting vacuum...end.
    transaction type: TPC-B (sort of)
    scaling factor: 20
    query mode: simple
    number of clients: 20
    number of threads: 1
    number of transactions per client: 2000
    number of transactions actually processed: 40000/40000
    latency average: 0.000 ms
    tps = 14458.676200 (including connections establishing)
    tps = 14466.893004 (excluding connections establishing)
注:TPS (transaction per second)

测试 2---->
	模拟80个并发用户，每个用户执行2000次事务。每种配置参数执行三次，记录TPS值。
[txdb@hdf-3 bin]$ ./pgbench -t 2000 -c 80 -U txdb_bi test
    starting vacuum...end.
    transaction type: TPC-B (sort of)
    scaling factor: 20
    query mode: simple
    number of clients: 80
    number of threads: 1
    number of transactions per client: 2000
    number of transactions actually processed: 160000/160000
    latency average: 0.000 ms
    tps = 18118.180133 (including connections establishing)
    tps = 18121.388063 (excluding connections establishing)

测试 3---->
	模拟80个并发用户，20个线程，每个用户执行2000次事务。每种配置参数执行三次，记录TPS值。
[txdb@hdf-3 bin]$ ./pgbench -t 2000 -c 80 -j 20 -U txdb_bi test
    starting vacuum...end.
    transaction type: TPC-B (sort of)
    scaling factor: 20
    query mode: simple
    number of clients: 80
    number of threads: 20
    number of transactions per client: 2000
    number of transactions actually processed: 160000/160000
    latency average: 0.000 ms
    tps = 28932.307442 (including connections establishing)
    tps = 28953.063817 (excluding connections establishing)

测试 4---->
	模拟200个并发用户，20个线程，每个用户执行2000次事务。每种配置参数执行三次，记录TPS值。
[txdb@hdf-3 bin]$ ./pgbench -t 2000 -c 200 -j 20 -U txdb_bi test
   starting vacuum...end.
   transaction type: TPC-B (sort of)
   scaling factor: 20
   query mode: simple
   number of clients: 200
   number of threads: 20
   number of transactions per client: 2000
   number of transactions actually processed: 400000/400000
   latency average: 0.000 ms
   tps = 24477.088313 (including connections establishing)
   tps = 24483.720066 (excluding connections establishing)

测试 5---->
	模拟200个并发用户，100个线程，每个用户执行2次事务。每种配置参数执行三次，记录TPS值。
[txdb@hdf-3 bin]$ ./pgbench -t 200 -c 200 -j 100 -U txdb_bi test
[txdb@hdf-3 bin]$  ./pgbench -t 200 -c 200 -j 100 -U txdb_bi test
    starting vacuum...end.
    transaction type: TPC-B (sort of)
    scaling factor: 20
    query mode: simple
    number of clients: 200
    number of threads: 100
    number of transactions per client: 200
    number of transactions actually processed: 40000/40000
    latency average: 0.000 ms
    tps = 25448.060826 (including connections establishing)
    tps = 25628.392815 (excluding connections establishing)